{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293c8854",
   "metadata": {},
   "source": [
    "## Σε αυτό το σημείο της εργασίες πρέπει να συγκρίνουμε τα 3 pipelines που χρησιμοποιήσαμε στο ερώτημα 1Β μεταξύ τους\n",
    "\n",
    "### Βήματα \n",
    "- Διαβάζουμε τα 3 ανακατσκευασμένα κείμενα\n",
    "- Κάνουμε σύγκριση"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67916b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text=\"\"\n",
    "pipeline1=\"\"\n",
    "pipeline2=\"\"\n",
    "pipeline3=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6c002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File content successfully loaded into original_text.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('../text_files/text1.txt', 'r', encoding='utf-8') as file:\n",
    "        original_text = file.read()\n",
    "    print(\"File content successfully loaded into original_text.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file 'text1.txt' was not found. Please make sure the file exists and the path is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5884379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File content successfully loaded into pipeline1.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"../text_files/first_pipeline_text1.txt\", 'r', encoding='utf-8') as file:\n",
    "        pipeline1 = file.read()\n",
    "    print(\"File content successfully loaded into pipeline1.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Please make sure the file exists and the path is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7297e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File content successfully loaded into pipeline2.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"../text_files/second_pipeline_text1.txt\", 'r', encoding='utf-8') as file:\n",
    "        pipeline2 = file.read()\n",
    "    print(\"File content successfully loaded into pipeline2.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Please make sure the file exists and the path is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08300657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File content successfully loaded into pipeline3.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"../text_files/third_pipeline_text1.txt\", 'r', encoding='utf-8') as file:\n",
    "        pipeline3 = file.read()\n",
    "    print(\"File content successfully loaded into pipeline3.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Please make sure the file exists and the path is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf1967",
   "metadata": {},
   "source": [
    "## Αρχικά θα συγκρίνουμε το πόσο διαφέρει η κάθε ανακατασκευή από το πρωτότυπο κείμενο με μέθοδο cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47799a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "def cosine_similarity_logic(original_text,pipeline):\n",
    "    sentences = [original_text, pipeline]\n",
    "    tokenized = [s.lower().split() for s in sentences]\n",
    "    vocab = sorted(set(word for doc in tokenized for word in doc))\n",
    "\n",
    "    tf_matrix = []\n",
    "    for doc in tokenized:\n",
    "        word_counts = Counter(doc)\n",
    "        tf_vector = [word_counts[word] / len(doc) for word in vocab]\n",
    "        tf_matrix.append(tf_vector)\n",
    "        \n",
    "        \n",
    "    N = len(tokenized) \n",
    "    df_vector = [sum(1 for doc in tokenized if word in doc) for word in vocab]\n",
    "    idf_vector = [np.log((N + 1) / (df + 1)) + 1 for df in df_vector]\n",
    "    tfidf_matrix = np.array(tf_matrix) * np.array(idf_vector)\n",
    "    \n",
    "    \n",
    "    similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
    "    print(f\"Cosine Similarity: {similarity}\")\n",
    "    \n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44a4578d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.909638830217755\n",
      "Cosine Similarity: 0.89091260117103\n",
      "Cosine Similarity: 0.6972463548228686\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity_logic(original_text,pipeline1)\n",
    "cosine_similarity_logic(original_text,pipeline2)\n",
    "cosine_similarity_logic(original_text,pipeline3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8e8a5",
   "metadata": {},
   "source": [
    "## Ground truth κείμενο από το gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d57772ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini=\"\"\"Happy Dragon Boat Festival! I hope you're celebrating safely and enjoying a wonderful time.\n",
    "\n",
    "Thank you for relaying our message to the doctor regarding his contract review. I actually received the approved message from the professor a couple of days ago. I'm very grateful for the professor's full support with our Springer proceedings publication.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf9ab8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.4912117852064911\n",
      "Cosine Similarity: 0.5029258632108523\n",
      "Cosine Similarity: 0.41372297221366444\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity_logic(gemini,pipeline1)\n",
    "cosine_similarity_logic(gemini,pipeline2)\n",
    "cosine_similarity_logic(gemini,pipeline3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801de5d",
   "metadata": {},
   "source": [
    "## Ground truth κείμενο από το deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b5279ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek=\"\"\"Today is our Dragon Boat Festival, a cherished celebration in Chinese culture. I wish you safety, joy, and prosperity during this time—may you enjoy the festivities as deeply as I hope for you.\n",
    "\n",
    "Thank you for sharing the message regarding the doctor’s contract review. I’ve already received the approved notice from the professor a few days ago and sincerely appreciate their unwavering support for our Springer proceedings publication.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8241150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.4240647916966377\n",
      "Cosine Similarity: 0.4173758627748884\n",
      "Cosine Similarity: 0.3736299567629978\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity_logic(deepseek,pipeline1)\n",
    "cosine_similarity_logic(deepseek,pipeline2)\n",
    "cosine_similarity_logic(deepseek,pipeline3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec1c99e",
   "metadata": {},
   "source": [
    "## Ground truth κείμενο από το chat gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a85a547",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gpt=\"\"\"Today is our Dragon Boat Festival, a special celebration in our Chinese culture. It is a time to honor and wish for safety and prosperity in our lives. I hope you also enjoy this festival, as I send you my warmest wishes.\n",
    "\n",
    "Thank you for your message and for conveying our words to the doctor regarding his upcoming contract review. This is important for all of us.\n",
    "\n",
    "I received this message to confirm the approval. Actually, the professor shared this with me a couple of days ago. I truly appreciate the professor’s full support for our Springer proceedings publication.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24b9a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6111970629422422\n",
      "Cosine Similarity: 0.6211219678610983\n",
      "Cosine Similarity: 0.49879739978083254\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity_logic(chat_gpt,pipeline1)\n",
    "cosine_similarity_logic(chat_gpt,pipeline2)\n",
    "cosine_similarity_logic(chat_gpt,pipeline3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fcb5bb",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6885c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f56053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Υπολογίζει την ομοιότητα συνημιτόνου μεταξύ δύο NumPy διανυσμάτων.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0 # Αποφυγή διαίρεσης με το μηδέν\n",
    "    \n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "692383ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Word2VecCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2VecCBOW, self).__init__()\n",
    "        # Το embedding layer: Μετατρέπει τα integer IDs των λέξεων σε πυκνά διανύσματα.\n",
    "        # vocab_size: Ο αριθμός των μοναδικών λέξεων στο λεξιλόγιό μας.\n",
    "        # embedding_dim: Η διάσταση του διανύσματος για κάθε λέξη (π.χ., 100, 300).\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Το γραμμικό (linear) layer εξόδου:\n",
    "        # Παίρνει το embedding του πλαισίου και προβλέπει την κεντρική λέξη\n",
    "        # δίνοντας ένα σκορ για κάθε λέξη στο λεξιλόγιο.\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: Ένα tensor με τα IDs των λέξεων του πλαισίου.\n",
    "        # Αναμενόμενο σχήμα: (batch_size, context_window_size)\n",
    "        \n",
    "        # Βήμα 1: Λάβετε τα embeddings για κάθε λέξη του πλαισίου.\n",
    "        # Το self.embeddings(inputs) θα επιστρέψει ένα tensor σχήματος:\n",
    "        # (batch_size, context_window_size, embedding_dim)\n",
    "        embeds = self.embeddings(inputs)\n",
    "        \n",
    "        # Βήμα 2: Υπολογίστε το embedding του πλαισίου.\n",
    "        # Για το CBOW, αυτό είναι συνήθως ο μέσος όρος των embeddings των λέξεων του πλαισίου.\n",
    "        # Το dim=1 σημαίνει ότι υπολογίζουμε το μέσο όρο κατά μήκος της διάστασης του \"πλαισίου\".\n",
    "        # Αποτέλεσμα: (batch_size, embedding_dim)\n",
    "        context_embed = torch.mean(embeds, dim=1)\n",
    "        \n",
    "        # Βήμα 3: Περάστε το embedding του πλαισίου από το output layer.\n",
    "        # Αυτό παράγει τα \"logits\" ή \"σκορ\" για κάθε λέξη στο λεξιλόγιο.\n",
    "        # Αποτέλεσμα: (batch_size, vocab_size)\n",
    "        output = self.output_layer(context_embed)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e45b95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_embedding(paragraph, model, word_to_idx):\n",
    "    \"\"\"\n",
    "    Δημιουργεί ένα ενιαίο διάνυσμα (embedding) για μια παράγραφο\n",
    "    κάνοντας τον μέσο όρο των embeddings των λέξεων που την απαρτίζουν.\n",
    "    Χρησιμοποιεί το embedding layer από το δικό σας μοντέλο.\n",
    "    \"\"\"\n",
    "    tokens = [word.lower() for word in word_tokenize(paragraph)]\n",
    "    \n",
    "    # Συλλέξτε τα IDs των λέξεων που υπάρχουν στο λεξιλόγιο\n",
    "    word_ids = []\n",
    "    for word in tokens:\n",
    "        if word in word_to_idx:\n",
    "            word_ids.append(word_to_idx[word])\n",
    "    \n",
    "    if not word_ids:\n",
    "        # Αν δεν βρέθηκαν λέξεις στο λεξιλόγιο, επιστρέψτε ένα διάνυσμα μηδενικών\n",
    "        # με τη σωστή διάσταση του embedding\n",
    "        return np.zeros(model.embeddings.embedding_dim)\n",
    "    \n",
    "    # Μετατροπή των IDs σε PyTorch tensor\n",
    "    # Προσθήκη μιας διάστασης batch (unsqueeze(0)) γιατί το embedding layer περιμένει batch\n",
    "    input_tensor = torch.tensor(word_ids, dtype=torch.long).unsqueeze(0) # (1, num_words)\n",
    "    \n",
    "    # Λάβετε τα embeddings από το embedding layer του μοντέλου\n",
    "    # Δεν χρειάζεται να περάσετε από ολόκληρο το forward pass του μοντέλου\n",
    "    # γιατί θέλουμε μόνο τα embeddings, όχι τις προβλέψεις του output layer.\n",
    "    with torch.no_grad(): # Δεν χρειαζόμαστε gradients για απλή ανάκτηση embeddings\n",
    "        word_embeds = model.embeddings(input_tensor) # (1, num_words, embedding_dim)\n",
    "    \n",
    "    # Υπολογίστε τον μέσο όρο των embeddings για να πάρετε το embedding της παραγράφου\n",
    "    paragraph_embedding = torch.mean(word_embeds, dim=1).squeeze(0) # (embedding_dim,)\n",
    "    \n",
    "    return paragraph_embedding.numpy() # Επιστροφή ως NumPy array για υπολογισμό ομοιότητας"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1400990d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δημιουργήθηκε λεξιλόγιο με 137 λέξεις.\n",
      "--------------------------------------------------\n",
      "Το μοντέλο Word2VecCBOW αρχικοποιήθηκε.\n",
      "Διαστάσεις embedding layer: torch.Size([137, 100])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Προετοιμασία Δεδομένων και Λεξιλογίου ---\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "all_paragraphs = [pipeline1, pipeline2, pipeline3]\n",
    "all_words = []\n",
    "for p in all_paragraphs:\n",
    "    all_words.extend([word.lower() for word in word_tokenize(p)]) # word_tokenize τώρα είναι defined\n",
    "\n",
    "word_to_idx = defaultdict(lambda: len(word_to_idx))\n",
    "idx_to_word = {}\n",
    "\n",
    "for word in all_words:\n",
    "    if word not in word_to_idx:\n",
    "        idx = word_to_idx[word]\n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "print(f\"Δημιουργήθηκε λεξιλόγιο με {VOCAB_SIZE} λέξεις.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 3. Αρχικοποίηση του Μοντέλου ---\n",
    "model = Word2VecCBOW(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "print(\"Το μοντέλο Word2VecCBOW αρχικοποιήθηκε.\")\n",
    "print(f\"Διαστάσεις embedding layer: {model.embeddings.weight.shape}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8e3ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Συγκρίσεις Παραγράφων με τη Δική σας Κλάση ---\n",
      "Παράγραφος 1: \"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safety and great in our lives.Hope you too, enjoy it as my deepest wishes.Thank you for your message to show our words to the doctor, as his next contract checking, to all of us.I got this message to see the approved message.In fact, I have received the message from the professor, to show me, this, a couple of days ago.I am very appreciated the full support of the professor, for our Springer proceedings publication.\"\n",
      "Παράγραφος 2: \"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safety and great in our lives.Hope you too, to enjoy it as my deepest wishes.Thank you for your message to show our words to the doctor, as his next contract checks, to all of us.I got this message to see the approved message.In fact, I have received the message from the professor, to show me, this, a couple of days ago.I am very grateful for the full support of the professor, for our Springer proceedings publication.\"\n",
      "Παράγραφος 3: \" Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives . Celebrations include dragon boat festivals, dragon boat celebrations, and dragon boat races, in Chinese culture . We are happy to celebrate the festival with safe, great and great celebrations . \"Hope you too, to enjoy it as my deepest wishes. Hope you too,\" she said. \"Enjoy it as I do. My deepest wishes.\" She added that she hopes you too enjoy it, as well as the rest of the world . Enjoy it, she said, and it's a good time for you to enjoy your life . Thank your message to show our words to the doctor, as his next contract checking, to all of us . \"Thank your message, as your next contract check is to all the us,\" she said. \"Please show us your words to him, as our next contract checks are to him\" I got this message to see the approved message. See it here for more information . Send it to iReport-in-depth . Use the weekly Newsquiz to test your knowledge of stories you saw on CNN iReport . Back to the page you came from . In fact, I have received the message from the .professor, to show me, this, a couple of days ago . The professor said he had been sent a message to show this to him . He said: \"In fact, he has received the . message from . the professor to show it to me, and I am happy to see it, and it is a good thing.\" I am very appreciated the full support of theprofessor, for our Springer proceedings publication . The Springer proceedings will be published by Springer Springer on October 31, 2015 . The full text of the Springer proceedings is published on October 30, 2015. The full transcript of the proceedings is available on Springer.com .\"\n",
      "--------------------------------------------------\n",
      "Ομοιότητα μεταξύ Παραγράφου 1 και Παραγράφου 2: 0.9912\n",
      "Ομοιότητα μεταξύ Παραγράφου 1 και Παραγράφου 3: 0.8258\n",
      "Ομοιότητα μεταξύ Παραγράφου 1 και Παραγράφου 4: 0.8280\n",
      "\n",
      "Σημείωση: Τα αποτελέσματα ομοιότητας εδώ είναι πιθανόν τυχαία ή πολύ αδύναμα,\n",
      "καθώς το μοντέλο Word2VecCBOW δεν έχει εκπαιδευτεί σε ουσιαστικά δεδομένα.\n",
      "Για να δείτε πραγματικά σημασιολογικά αποτελέσματα, το μοντέλο χρειάζεται εκτενή εκπαίδευση.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Συγκρίσεις Παραγράφων με τη Δική σας Κλάση ---\")\n",
    "\n",
    "embedding1 = get_paragraph_embedding(pipeline1, model, word_to_idx)\n",
    "embedding2 = get_paragraph_embedding(pipeline2, model, word_to_idx)\n",
    "embedding3 = get_paragraph_embedding(pipeline3, model, word_to_idx)\n",
    "\n",
    "print(f\"Παράγραφος 1: \\\"{pipeline1}\\\"\")\n",
    "print(f\"Παράγραφος 2: \\\"{pipeline2}\\\"\")\n",
    "print(f\"Παράγραφος 3: \\\"{pipeline3}\\\"\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "similarity_1_2 = cosine_similarity(embedding1, embedding2)\n",
    "print(f\"Ομοιότητα μεταξύ Παραγράφου 1 και Παραγράφου 2: {similarity_1_2:.4f}\")\n",
    "\n",
    "similarity_1_3 = cosine_similarity(embedding1, embedding3)\n",
    "print(f\"Ομοιότητα μεταξύ Παραγράφου 1 και Παραγράφου 3: {similarity_1_3:.4f}\")\n",
    "\n",
    "similarity_1_4 = cosine_similarity(embedding2, embedding3)\n",
    "print(f\"Ομοιότητα μεταξύ Παραγράφου 1 και Παραγράφου 4: {similarity_1_4:.4f}\")\n",
    "\n",
    "print(\"\\nΣημείωση: Τα αποτελέσματα ομοιότητας εδώ είναι πιθανόν τυχαία ή πολύ αδύναμα,\")\n",
    "print(\"καθώς το μοντέλο Word2VecCBOW δεν έχει εκπαιδευτεί σε ουσιαστικά δεδομένα.\")\n",
    "print(\"Για να δείτε πραγματικά σημασιολογικά αποτελέσματα, το μοντέλο χρειάζεται εκτενή εκπαίδευση.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
