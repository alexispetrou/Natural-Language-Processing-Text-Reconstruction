{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b96c13e",
   "metadata": {},
   "source": [
    "## Προσθέτουμε τις απαραίτητες βιβλιοθήκες"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89614eb8",
   "metadata": {},
   "source": [
    "## Σημείωση : Η sentence_transformers χρησιμοποιεί pre-trained μοντέλα για την παραγωγή embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1030398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01d824",
   "metadata": {},
   "source": [
    "## Κώδικας για να ανοίξουμε και να συλλέξουμε το αρχικό κείμενο αλλά και τα αποτελέσματα των pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3e3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_files(path):  \n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            print(\"File content successfully loaded.\")\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The file  was not found. Please make sure the file exists and the path is correct.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9f1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File content successfully loaded.\n",
      "File content successfully loaded.\n",
      "File content successfully loaded.\n",
      "File content successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "paths=[\"../text_files/text2.txt\",\"../text_files/first_pipeline_text2.txt\",\"../text_files/second_pipeline_text2.txt\",\"../text_files/third_pipeline_text2.txt\"]\n",
    "texts=[]\n",
    "for path in paths:\n",
    "    texts.append(take_files(path))\n",
    "original_text=texts[0]\n",
    "pipeline1=texts[1]\n",
    "pipeline2=texts[2]\n",
    "pipeline3=texts[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf326e1b",
   "metadata": {},
   "source": [
    "## Αποθήκευση των ground truth's με βάση τα οποία θα γίνει η σύγκριση\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0e27323",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = \"\"\"During our last discussion, I mentioned the new submission we have been waiting for since last autumn, noting that the recent updates were confusing because they appeared to be missing the full feedback from either the reviewer or the editor. Despite some recent delays and a lack of communication, I firmly believe the team tried their absolute best and cooperated effectively, and we should all be grateful for their efforts which led to the paper's acceptance, finalized by the Springer link that arrived last week. Regarding the final steps, could you please remind me if the doctor still plans to edit the acknowledgments section before sending the paper again? I have not seen a final version of that part, though I apologize if I simply missed it. Let's ensure everyone is doing well, celebrate this successful outcome, and begin planning our future targets.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e215d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = \"\"\"During our final discussion, I informed him about the new paper submission—the one we had been anticipating since last autumn. However, the recent updates regarding this submission were confusing, as they did not appear to include the reviewers' full feedback, or possibly the editor's comments. Despite the recent delays and reduced communication from the team, I believe they genuinely put forth their best effort for the paper and our collaboration. Consequently, we should all be grateful for the paper's acceptance and the team's persistent work, especially now that the Springer link has finally become available last week. Also, please remind me if the doctor still plans to make edits to the acknowledgments section before he resubmits it, as I haven't seen the final version of that part yet—I apologize if I missed it. Overall, let's ensure everything is finalized, and then celebrate this outcome with strong coffee and discussions about future targets.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1cb0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gpt=\"\"\"During our final discussion, I updated him on the new submission we’ve been awaiting since last autumn. Although the most recent update was confusing—because it did not include the full reviewer or editor feedback—I believe our team, despite some delays and reduced communication lately, has been fully committed to the paper and to cooperating throughout the process. We should all be grateful for their hard work and for the acceptance, and I was pleased to see the Springer link finally go live last week. Please remind me whether the doctor still plans to edit the acknowledgments section before sending the final version again, as I have not yet seen it; I apologize if I missed it. Overall, let’s make sure everything is in order, celebrate this success with strong coffee, and set our sights on future goals.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0752cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    \"pipeline1\": pipeline1,\n",
    "    \"pipeline2\": pipeline2,\n",
    "    \"pipeline3\": pipeline3,\n",
    "}\n",
    "\n",
    "llms = {\n",
    "    \"chatgpt\":  chat_gpt,\n",
    "    \"gemini\":   gemini,\n",
    "    \"deepseek\": deepseek,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a1e19b",
   "metadata": {},
   "source": [
    "## Το bert_model υπολογίζει bert score και το embed_model χρησιμοποιεί έτοιμα embeddings και cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf36a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model  = \"bert-base-uncased\"                  \n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd5f2fc",
   "metadata": {},
   "source": [
    "## Φτιάχνουμε ένα λεξικό embeddings,όπου key το όνομα του αντικειμένου και value η κωδικοποίηση του σε embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb51552",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = { **pipelines, **llms }\n",
    "embeddings = {\n",
    "    name: embed_model.encode(text, convert_to_tensor=True)\n",
    "    for name, text in all_texts.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1d3d5",
   "metadata": {},
   "source": [
    "## Υπολογισμός Σκορ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a94890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e1b4ac83e2492a91371c30d0246d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = []\n",
    "for l_name, l_text in llms.items():\n",
    "    for p_name, p_text in pipelines.items():\n",
    "        # BERTScore (wrap in lists)\n",
    "        P, R, F1 = score([p_text], [l_text],\n",
    "                         lang=\"en\",\n",
    "                         model_type=bert_model,\n",
    "                         verbose=False,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        bP, bR, bF1 = float(P.mean()), float(R.mean()), float(F1.mean())\n",
    "\n",
    "        # Embedding cosine similarity\n",
    "        pe = embeddings[p_name]\n",
    "        le = embeddings[l_name]\n",
    "        cos = (pe @ le) / (pe.norm() * le.norm())\n",
    "\n",
    "        rows.append({\n",
    "            \"Pipeline\":       p_name,\n",
    "            \"LLM\":            l_name,\n",
    "            \"BERTScore_P\":    bP,\n",
    "            \"BERTScore_R\":    bR,\n",
    "            \"BERTScore_F1\":   bF1,\n",
    "            \"Embed_Cosine\":   float(cos)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc0be69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pipeline</th>\n",
       "      <th>LLM</th>\n",
       "      <th>BERTScore_P</th>\n",
       "      <th>BERTScore_R</th>\n",
       "      <th>BERTScore_F1</th>\n",
       "      <th>Embed_Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pipeline1</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>0.761817</td>\n",
       "      <td>0.773370</td>\n",
       "      <td>0.767550</td>\n",
       "      <td>0.944707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pipeline2</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>0.759986</td>\n",
       "      <td>0.772196</td>\n",
       "      <td>0.766042</td>\n",
       "      <td>0.945670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pipeline3</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>0.594978</td>\n",
       "      <td>0.682979</td>\n",
       "      <td>0.635949</td>\n",
       "      <td>0.825477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pipeline1</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.738055</td>\n",
       "      <td>0.749095</td>\n",
       "      <td>0.743534</td>\n",
       "      <td>0.914268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pipeline2</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.737574</td>\n",
       "      <td>0.747127</td>\n",
       "      <td>0.742320</td>\n",
       "      <td>0.902502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pipeline3</td>\n",
       "      <td>gemini</td>\n",
       "      <td>0.588306</td>\n",
       "      <td>0.662066</td>\n",
       "      <td>0.623010</td>\n",
       "      <td>0.817400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pipeline1</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>0.777190</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.771486</td>\n",
       "      <td>0.937202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pipeline2</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>0.770333</td>\n",
       "      <td>0.760873</td>\n",
       "      <td>0.765574</td>\n",
       "      <td>0.938816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pipeline3</td>\n",
       "      <td>deepseek</td>\n",
       "      <td>0.613579</td>\n",
       "      <td>0.693041</td>\n",
       "      <td>0.650894</td>\n",
       "      <td>0.829554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pipeline       LLM  BERTScore_P  BERTScore_R  BERTScore_F1  Embed_Cosine\n",
       "0  pipeline1   chatgpt     0.761817     0.773370      0.767550      0.944707\n",
       "1  pipeline2   chatgpt     0.759986     0.772196      0.766042      0.945670\n",
       "2  pipeline3   chatgpt     0.594978     0.682979      0.635949      0.825477\n",
       "3  pipeline1    gemini     0.738055     0.749095      0.743534      0.914268\n",
       "4  pipeline2    gemini     0.737574     0.747127      0.742320      0.902502\n",
       "5  pipeline3    gemini     0.588306     0.662066      0.623010      0.817400\n",
       "6  pipeline1  deepseek     0.777190     0.765865      0.771486      0.937202\n",
       "7  pipeline2  deepseek     0.770333     0.760873      0.765574      0.938816\n",
       "8  pipeline3  deepseek     0.613579     0.693041      0.650894      0.829554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b479357",
   "metadata": {},
   "source": [
    "### Σύντομη Επεξήγηση Μετρικών\n",
    "\n",
    "- **BERTScore Precision (P)**  \n",
    "  Από τα token που παράγει το pipeline, ποιο ποσοστό βρίσκει “αντίστοιχα” token μέσα στην έξοδο του LLM. Μετράει πόσο «στοχευμένη» είναι η παραγωγή του pipeline σε σχέση με την αναφορά(ground truth).\n",
    "\n",
    "- **BERTScore Recall (R)**  \n",
    "  Από τα token που έχει το LLM, ποιο ποσοστό καλύπτεται από τα token του pipeline. Δείχνει πόσο πλήρες είναι το pipeline σε σχέση με την αναφορά(ground truth).\n",
    "\n",
    "- **BERTScore F1**  \n",
    "  Ο  μέσος του Precision και του Recall. Ισορροπεί την ποιότητα (P) με την κάλυψη (R) σε ένα ενιαίο σκορ 0-1.\n",
    "\n",
    "- **Cosine Similarity (Embed_Cosine)**  \n",
    "  Συνητινιμική ομοιότητα ανάμεσα στα ολόκληρα embeddings (διάνυσμα) δύο κειμένων. Τιμή 1 σημαίνει ταυτόσημη κατεύθυνση, 0 σημαίνει ορθογώνια (καμιά σχέση).  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
